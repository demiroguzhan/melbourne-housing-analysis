---
title: "Melbourne Housing"
output: 
  github_document:
    toc: true
    number_sections: true
---

# Importing Dataset / Pre-analysis

We are importing our dataset, using "read.csv" fuction.
I decided to use strings as non-factor objects because i will set all the variables by myself after doing some analysis on data.
Since we have some cells with “#N/A” text in it, R doesn't see them as NA values.
To avoid them, we are using "na.strings: parameter to import them as NA's.

```{r}
housing_dataset <- read.csv("melbourne_data.csv",
                            stringsAsFactors = FALSE, na.strings = "#N/A")
```

Before doing any cleaning and altering, we are checking structure and summary of the data to have some understanding about
our data that we will analyze.

```{r}
str(housing_dataset)
```

When we check the structure of our data set, we can see there is a column named "X" which is not necessary since we already have
built in numbering system in R. We have lots of columns which has “chr” type which looks incorrect.
Since they are character, it won't be healthy to check summary of the dataset now. So we will work with our column types first.

# Preparing Columns for Analysis

## X Column

As we mentioned above we won't use column X. According to that, we are removing that column from our dataset for further analysis.

```{r}
housing_dataset$X <- NULL
```

## Date Column

We are formatting the “Date” column to Date object, looking into the new structure and summary to check everything is working
as what we wanted. Fortunately, we had no NA values in “Date” column.

```{r}
housing_dataset$Date <- as.Date(housing_dataset$Date, format = "%d/%m/%Y")
```

```{r}
str(housing_dataset$Date)
summary(housing_dataset$Date)
```

## Type Column

We are renaming strings for better explanation and factorizing those values.
Then, checking levels of created factor to be sure everything is alright.
Looking into the new structure and summary to check everything looks like as what we wanted.
We had no NA values in “Type” column too.

```{r}
housing_dataset$Type[housing_dataset$Type == "h"] <- "House"
housing_dataset$Type[housing_dataset$Type == "u"] <- "Unit/Duplex"
housing_dataset$Type[housing_dataset$Type == "t"] <- "Townhouse"
housing_dataset$Type <- factor(housing_dataset$Type)
```

```{r}
levels(factor(housing_dataset$Type))
str(housing_dataset$Type)
summary(housing_dataset$Type)
```

## Price Column

We are formatting “Price” column to integer since we don't have any double values in Price column.
Then, we are checking the structure, summary and NA values in the “Price” column.
We have 7610 missing values on “Price” column, we will work on them later.

```{r, warning=FALSE}
housing_dataset$Price <- as.integer(housing_dataset$Price)
```

```{r}
str(housing_dataset$Price)
summary(housing_dataset$Price)
```

## Landsize and BuildingArea Columns

We are formatting “Landsize” and “BuildingArea” columns to integer since they are sharing the same structure and
we don't have any double values in them.
Then, we are checking the structure, summary and NA values in those columns.
We have 11810 missing values on “Landsize” and 21115 missing values on “BuildingArea” column according to summary of the column.

```{r, warning=FALSE}
housing_dataset$Landsize <- as.integer(housing_dataset$Landsize)
housing_dataset$BuildingArea <- as.integer(housing_dataset$BuildingArea)
```

```{r}
str(housing_dataset$Landsize)
summary(housing_dataset$Landsize)
```

```{r}
str(housing_dataset$BuildingArea)
summary(housing_dataset$BuildingArea)
```


## Rooms, Bathroom, Car Columns

We will apply the same approach to “Rooms”, “Bathroom” and “Car” columns since they have identical structure in terms of data.
First of all, we will format them to integer and then we will check their structures and summaries one by one.
Fortunately, we have no NA values in “Rooms” column but we have 8226 missing values in “Bathroom” and 8728 missing values in “Car” column.

```{r, warning=FALSE}
housing_dataset$Rooms <- as.integer(housing_dataset$Rooms)
housing_dataset$Bathroom <- as.integer(housing_dataset$Bathroom)
housing_dataset$Car <- as.integer(housing_dataset$Car)
```

```{r}
str(housing_dataset$Rooms)
summary(housing_dataset$Rooms)
```

```{r}
str(housing_dataset$Bathroom)
summary(housing_dataset$Bathroom)
```

```{r}
str(housing_dataset$Car)
summary(housing_dataset$Car)
```

## YearBuilt Column

We will format “YearBuilt” column to integer.
It will help us while dealing with outliers in the column. 
Since we don't have any month and day information in this column, we don't need to use it as Date object.
After formatting, we are checking the structure and summary of this column.
We have 19306 missing values in this column.

```{r, warning=FALSE}
housing_dataset$YearBuilt <- as.integer(housing_dataset$YearBuilt)
```
```{r}
str(housing_dataset$YearBuilt)
summary(housing_dataset$YearBuilt)
```

## Distance Column
We are formatting “Distance” column to numeric since we have digits in those columns.
Then we are checking structure and summary as always.
We have just 1 cell missing in “Distance” column.

```{r, warning=FALSE}
housing_dataset$Distance <- as.numeric(housing_dataset$Distance)
```

```{r}
str(housing_dataset$Distance)
summary(housing_dataset$Distance)
```

## Regionname Column
We are factorizing the “Regionname” column and checking its levels.
Then, we look into structure and summary of it to be sure everything is correct.
We have 3 cells missing in “Regionname” column.

```{r, warning=FALSE}
housing_dataset$Regionname <- factor(housing_dataset$Regionname)
```

```{r}
levels(factor(housing_dataset$Regionname))
str(housing_dataset$Regionname)
summary(housing_dataset$Regionname)
```

## PropertyCount Column
We will factorize the “PropertyCount” column because those values actually are not different from each other.
There are several property areas in different regions which has exactly the same amount of properties.
So, they show us “which” property area that property belongs to.
According to that, we will use that variable as factor, instead of integer. Then we are checking its structure and summary.
We have 3 missing rows in “PropertyCount” column, same as “Regionname”

```{r, warning=FALSE}
housing_dataset$Propertycount <- factor(housing_dataset$Propertycount)
```

```{r, results='hide'}
levels(factor(housing_dataset$Propertycount))
```

*Output of levels are not included since there are so many different levels. You can remove 'results' parameter in code block above to see output.*

```{r}
str(housing_dataset$Propertycount)
summary(housing_dataset$Propertycount)
```

# Replacing Missing Values

First, let's have a look the data we have obtained during the pre-process.

Date Column: 0 missing rows
Type Column: 0 missing rows
Price Column: 7610 missing rows
Landsize Column: 11810 missing rows
BuildingArea Column: 21115 missing rows
Rooms Column: 0 missing rows
Bathroom Column: 8226 missing rows
Car Column: 8728 missing rows 
earBuilt Column: 19306 missing rows
Distance Column: 1 missing rows
Regionname Column: 3 missing rows
Propertycount Column: 3 missing rows

When we look at the table above, we can see that the Distance, Regionname and Propertycount columns have very little missing data.
Starting from this data will be the easiest step at this point.

```{r}
housing_dataset[is.na(housing_dataset$Distance), ]
housing_dataset[is.na(housing_dataset$Regionname), ]
housing_dataset[is.na(housing_dataset$Propertycount), ]
```

It seems that these 3 columns are missing 3 rows in common.
We have the “Distance” data in the 2 missing rows, and as we know, the “Distance”, “Regionname”, and “Propertycount” columns
are there to provide the same information: the area where the property is located.
With that in mind, we can use this “Distance” data to predict other columns.

First, let's take a look at the regions where the “Distance” data is 5.1.

```{r}
summary(housing_dataset$Regionname[housing_dataset$Distance == 5.1])
```

When we perform some mathematical operations on the above data, we obtain the Regionname value of the first row can be
Western Metropolitan with a probability of 69.1%, or Southern Metropolitan with a probability of 30.9%.
Accordingly, instead of deleting this data, we can add it to the analysis by restoring it.

```{r}
housing_dataset[18524, "Regionname"] <- "Western Metropolitan"
```

Now let's do the same steps for second line.

```{r}
summary(housing_dataset$Regionname[housing_dataset$Distance == 7.7])
```

When we do the same operations for the second line, we get the Southern Metropolitan value with a probability of 91.9%
and a Western Metropolitan value with a probability of 8.1%.
Based on this information, we can assume the Regionname of second line to be Southern Metropolitan and include it in the analysis.

```{r}
housing_dataset[26889, "Regionname"] <- "Southern Metropolitan"
```

We can do a similar approach for the “Propertycount” column.
If we examine the property area where the properties are located in that region and include the house prices in this examination,
we can find where these properties belong.

```{r}
summary(housing_dataset$Propertycount[housing_dataset$Distance == 5.1 &
                                      housing_dataset$Regionname == "Western Metropolitan"])
```

When we look at the table above, we can say that this property is more likely to be located in the area with 7570 properties.
However, it is also useful to compare the price.

```{r}
housing_dataset[18524, "Price"]
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 5.1 &
                              housing_dataset$Regionname == "Western Metropolitan" &
                              housing_dataset$Propertycount == 7570])
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 5.1 &
                              housing_dataset$Regionname == "Western Metropolitan" &
                              housing_dataset$Propertycount == 2417])
```

The value of this property is defined as 710.000 units.
When we look at the mean and median price values in two different fields, the arrows again shows the field with 7570 properties.
With this information, we edit the Propertycount of this property as 7570 and include it in our analysis.

```{r}
housing_dataset[18524, "Propertycount"] <- 7570
```

Now let's do the same approach for the second row.

```{r}
summary(housing_dataset$Propertycount[housing_dataset$Distance == 7.7 &
                                      housing_dataset$Regionname == "Southern Metropolitan"])
```

It is very difficult to make a prediction based on the above table. 
There are almost equal number of properties in 2 different regions.
Therefore, we will make a prediction based on the price values.

```{r}
housing_dataset[26889, "Price"]
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 7.7 &
                              housing_dataset$Regionname == "Southern Metropolitan" &
                              housing_dataset$Propertycount == 8920])
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 7.7 &
                              housing_dataset$Regionname == "Southern Metropolitan" &
                              housing_dataset$Propertycount == 8989])
```

The value of our second property is defined as 825.000 units.
When we look at the mean and median values in two different area, we can easily say that this property belongs to the second area.
Therefore, we change the “Propertycount” data of this property to 8989.

```{r}
housing_dataset[26889, "Propertycount"] <- 8989
```

We recovered two of the 3 missing rows. Let's take a look at the third one.

```{r}
housing_dataset[29484, ]
```

Unfortunately there is nothing we can do for this row.
While we only have the number of rooms and the price, there is no way we can recover such missing data.
Therefore, we will not include this row in our analysis.

```{r}
housing_dataset <- housing_dataset[-c(29484), ]
```

Before concluding this section, let's take a final look at a summary of our data.

```{r}
summary(housing_dataset)
```

# Outliers

Since the other columns has high amount of missing values, before doing any approach to them we will get rid of the outliers
which will help us to eliminate some of the missing values and give us an opportunity to predict some of those missing values.
With outliers, our predictions might get affected and give misleading results.

Values that we call “outlier”, which are very high or very low than the other values in the category it is in,
may affect the values we will obtain from that category and may cause us to make an incorrect analysis.
These data may be true or false, but we will exclude this data from our analysis as it will have a negative impact
on our analysis no matter what. We will make use of the quantile function and the filtering method when applying this process.

## Price Column Outliers

First of all, let's examine the data in 1% and 99% percentile and make sure there is no problem.

```{r}
quantile(housing_dataset$Price, 0.99, na.rm = TRUE)
quantile(housing_dataset$Price, 0.01, na.rm = TRUE)
```

For “Price” column we got 3.400.550 for upper and 310.000 for lower values.
Upper percentile value seems higher than expected so we might need to do something different for “Price” column.
To be sure, we will check the “Price” values in histogram plot.
To do that, we will use ggplot2 and scales package and create a histogram chart from the data we have.
To get a better chart, we increase the number of bins to 100 and focus on the X-axis between 0 and 5,000,000.
Through the Scales package, we ensure that the data on the X axis appear in a smoother format.

```{r}
library(ggplot2)
library(scales)
```

```{r, warning=FALSE}
ggplot(data=housing_dataset, aes(x=Price)) +
  geom_histogram(bins=100) +
  coord_cartesian(xlim=c(0,5000000)) +
  scale_x_continuous(labels = unit_format(unit = "M", scale = 1e-6))
```

According to plot we got, we can see there are prices goes up to 5 million in the plot,
so we can say that 3.400.550 is an acceptable value for price.
According to that information, we can exclude data below 1% and above 99% from our analysis.
Since we do not want to get rid of missing values yet, we include missing values in our filter via OR operator.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Price <= quantile(housing_dataset$Price, 0.99, na.rm = TRUE) | is.na(housing_dataset$Price)),]
housing_dataset <- housing_dataset[(housing_dataset$Price >= quantile(housing_dataset$Price, 0.01, na.rm = TRUE) | is.na(housing_dataset$Price)),]
```
## Landsize Column Outliers

Let's first take a look at the quantile values, as we just did.

```{r}
quantile(housing_dataset$Landsize, 0.99, na.rm = TRUE)
quantile(housing_dataset$Landsize, 0.01, na.rm = TRUE)
```

In “Landsize” column, we got 2980.52 for upper value and 0 for lower value.
Now, we need to do something different because “Landsize” cannot be 0, that means there is something wrong in our data.
To be sure, we will check the distribution of “Landsize” column in histogram.

```{r, warning=FALSE}
ggplot(data=housing_dataset, aes(x=Landsize)) +
  geom_histogram(bins=nrow(housing_dataset)) +
  coord_cartesian(xlim=c(0,500), ylim=c(0,5000))
```

As we can see in the histogram, there are more than 2000 rows which has 0 value in Landsize row.
Because of that, we are getting 0 as lower percentile. To avoid that, we will remove the lines with 0 values in their Landsize column.

```{r}
housing_dataset <- housing_dataset[housing_dataset$Landsize > 0 | is.na(housing_dataset$Landsize), ]
```

```{r}
quantile(housing_dataset$Landsize, 0.99, na.rm = TRUE)
quantile(housing_dataset$Landsize, 0.01, na.rm = TRUE)
```

After that operation, we got 81 as lower percentile and 3439.6 as upper percentile which is acceptable.
Now we can apply the same method that we were using to “Landsize” column.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Landsize < quantile(housing_dataset$Landsize, 0.99, na.rm = TRUE) | is.na(housing_dataset$Landsize)),]
housing_dataset <- housing_dataset[(housing_dataset$Landsize > quantile(housing_dataset$Landsize, 0.01, na.rm = TRUE) | is.na(housing_dataset$Landsize)),]
```

## BuildingArea Column Outliers

As always, we are starting by checking percentile values.

```{r}
quantile(housing_dataset$BuildingArea, 0.99, na.rm = TRUE)
quantile(housing_dataset$BuildingArea, 0.01, na.rm = TRUE)
```

We have similar problem with “BuildingArea” column.
We are getting 484.42 as upper value but 3.43 as lower value and we know that BuildingArea cannot be 3.43,
so probably there are some incorrect values which is stacked on low values.
To approve that we will check BuildingArea column in chart.

```{r}
ggplot(data=housing_dataset, aes(x=BuildingArea)) +
  geom_histogram(bins=nrow(housing_dataset)) +
  coord_cartesian(xlim=c(0,200), ylim=c(0,300))
```

As we can see in the chart, there are stacking near value 0. We will zoom more to see the problem more detailed.

```{r, warning=FALSE}
ggplot(data=housing_dataset, aes(x=BuildingArea)) +
  geom_histogram(bins=nrow(housing_dataset)) +
  coord_cartesian(xlim=c(0,10), ylim=c(0,100))
```

We can see some stacking between 0 and 7, so we will cut the values which has less than 7. After that, we can continue.

```{r}
housing_dataset <- housing_dataset[housing_dataset$BuildingArea > 5 | is.na(housing_dataset$BuildingArea), ]
```

```{r}
quantile(housing_dataset$BuildingArea, 0.99, na.rm = TRUE)
quantile(housing_dataset$BuildingArea, 0.01, na.rm = TRUE)
```

Now, we got 43.14 as lower value and 488.72 as upper value which is acceptable. Now we can apply our method on BuildingArea column.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$BuildingArea < quantile(housing_dataset$BuildingArea, 0.99, na.rm = TRUE) | is.na(housing_dataset$BuildingArea)),]
housing_dataset <- housing_dataset[(housing_dataset$BuildingArea > quantile(housing_dataset$BuildingArea, 0.01, na.rm = TRUE) | is.na(housing_dataset$BuildingArea)),]
```

We detected some rows while checking our data frame which has bigger building areas than landsizes which is unrealistic.
So, we will exclude those rows aswell.

```{r}
housing_dataset <- housing_dataset[housing_dataset$BuildingArea < housing_dataset$Landsize | is.na(housing_dataset$BuildingArea) | is.na(housing_dataset$Landsize),]
```

## Rooms Column Outliers

Let's start with checking our percentile values.

```{r}
quantile(housing_dataset$Rooms, 0.99, na.rm = TRUE)
quantile(housing_dataset$Rooms, 0.01, na.rm = TRUE)
```

When we check the quantile values for Rooms column, we we will see 5 for upper and 1 for lower percentile which is acceptable
and realistic, so we will apply the same approach but we won't include is.na() filtersince Rooms column doesn't have
any missing values in it.

```{r}
housing_dataset <- housing_dataset[housing_dataset$Rooms <= quantile(housing_dataset$Rooms, 0.99, na.rm = TRUE),]
housing_dataset <- housing_dataset[housing_dataset$Rooms >= quantile(housing_dataset$Rooms, 0.01, na.rm = TRUE),]
```

## Bathroom Column Outliers

Again, we look at quantile values first.

```{r}
quantile(housing_dataset$Bathroom, 0.99, na.rm = TRUE)
quantile(housing_dataset$Bathroom, 0.01, na.rm = TRUE)
```

When we get the %99 and %1 percentile values of “Bathroom” values, it returns us 4 and 1.
We will exclude properties which has less than 1 and more than 4 bathrooms in it.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Bathroom <= quantile(housing_dataset$Bathroom, 0.99, na.rm = TRUE) | is.na(housing_dataset$Bathroom)),]
housing_dataset <- housing_dataset[(housing_dataset$Bathroom >= quantile(housing_dataset$Bathroom, 0.01, na.rm = TRUE) | is.na(housing_dataset$Bathroom)),]
```

## Car Column Outliers

We start with quantile as usual:

```{r}
quantile(housing_dataset$Car, 0.99, na.rm = TRUE)
quantile(housing_dataset$Car, 0.01, na.rm = TRUE)
```

In “Car” column, we get the 5 for upper percentile and 0 for lower percentile.
It's acceptable because some houses may not have car parks.
According to that we will apply the same approach to “Car” column.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Car <= quantile(housing_dataset$Car, 0.99, na.rm = TRUE) | is.na(housing_dataset$Car)),]
housing_dataset <- housing_dataset[(housing_dataset$Car >= quantile(housing_dataset$Car, 0.01, na.rm = TRUE) | is.na(housing_dataset$Car)),]
```

## YearBuilt Column Outliers

We look at the quantile values one last time.

```{r}
quantile(housing_dataset$YearBuilt, 0.99, na.rm = TRUE)
quantile(housing_dataset$YearBuilt, 0.01, na.rm = TRUE)
```

We are applying same method to YearBuilt column because we get meaningful quantile results like 1880 to 2016.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$YearBuilt <= quantile(housing_dataset$YearBuilt, 0.99, na.rm = TRUE) | is.na(housing_dataset$YearBuilt)),]
housing_dataset <- housing_dataset[(housing_dataset$YearBuilt >= quantile(housing_dataset$YearBuilt, 0.01, na.rm = TRUE) | is.na(housing_dataset$YearBuilt)),]
```

Before we end this section, let's look at the summary of our data again and make sure that everything is as we want.

```{r}
summary(housing_dataset)
```

Everything looks good. Then we can begin to dealing with our missing data.







```{r, echo=FALSE, results='hide', message=FALSE}
knitr::purl(input = "melbourne-housing.Rmd", output = "melbourne-housing-no-doc.R",documentation = 0)
```