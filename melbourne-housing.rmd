---
title: "Melbourne Housing"
output: 
  github_document:
    toc: true
    number_sections: true
---

# Importing Dataset / Pre-analysis

We are importing our dataset by using "read.csv" fuction.
I decided to use strings as non-factor objects because I will set all the variables by myself after doing some analysis on data.
Since we have some cells with “#N/A” text in it (according the info we got by analysing our dataset externally), R doesn't see them as NA values.
To avoid them, we will be using "na.strings" parameter to import them as NA's.

```{r}
housing_dataset <- read.csv("melbourne_data.csv",
                            stringsAsFactors = FALSE, na.strings = "#N/A")
```

Before doing any cleaning and altering on dataset, we will be checking structure and summary of the dataset to have some idea about it.

```{r}
str(housing_dataset)
```

When we check the structure of our dataset, we can see there is a column named "X" which is unnecessary since we already have
built in numbering system in R. We have lots of columns which is “chr” type and it looks incorrect.
Since they are character, it won't be healthy to check summary of the dataset now. So we will work with our column types first.

# Preparing Columns for Analysis

## X Column

As we mentioned above we won't use column X. According to that, we will be removing that column from our dataset for further analysis.

```{r}
housing_dataset$X <- NULL
```

## Date Column

We are formatting the “Date” column to Date object, looking into the new structure and summary to check everything is working
as what we wanted. Fortunately, we had no NA values in “Date” column.

```{r}
housing_dataset$Date <- as.Date(housing_dataset$Date, format = "%d/%m/%Y")
```

```{r}
str(housing_dataset$Date)
summary(housing_dataset$Date)
```

## Type Column

We will be renaming strings for better explanation and factorizing those values.
Then we will check levels of created factor to ensure everything is alright.
After factorising, we will be looking into the new structure and summary to ensure everything looks like as what we wanted.
By checking our summary report, we can see that we have no NA values in “Type” column too.

```{r}
housing_dataset$Type[housing_dataset$Type == "h"] <- "House"
housing_dataset$Type[housing_dataset$Type == "u"] <- "Unit/Duplex"
housing_dataset$Type[housing_dataset$Type == "t"] <- "Townhouse"
housing_dataset$Type <- factor(housing_dataset$Type)
```

```{r}
levels(factor(housing_dataset$Type))
str(housing_dataset$Type)
summary(housing_dataset$Type)
```

## Price Column

We will be formatting “Price” column to integer since we don't have any double values in Price column.
Then we will be checking the structure, summary in the “Price” column.
According to summary we got, we have 7610 missing values on “Price” column which we will work on them later.

```{r, warning=FALSE}
housing_dataset$Price <- as.integer(housing_dataset$Price)
```

```{r}
str(housing_dataset$Price)
summary(housing_dataset$Price)
```

## Landsize and BuildingArea Columns

We will be formatting “Landsize” and “BuildingArea” columns to integer since they are sharing the same structure and
we don't have any double values in them.
Then, we will check the structure, summary and extract NA values by reading summary report.
We have 11810 missing values on “Landsize” and 21115 missing values on “BuildingArea” column.

```{r, warning=FALSE}
housing_dataset$Landsize <- as.integer(housing_dataset$Landsize)
housing_dataset$BuildingArea <- as.integer(housing_dataset$BuildingArea)
```

```{r}
str(housing_dataset$Landsize)
summary(housing_dataset$Landsize)
```

```{r}
str(housing_dataset$BuildingArea)
summary(housing_dataset$BuildingArea)
```


## Rooms, Bathroom, Car Columns

We will apply the same approach to “Rooms”, “Bathroom”, and “Car” columns since they have identical structure in terms of data.
First of all, we will format them to integer and then we will be checking their structures and summaries one by one.
Fortunately, we have no NA values in “Rooms” column but we have 8226 missing values in “Bathroom” and 8728 missing values in “Car” column.

```{r, warning=FALSE}
housing_dataset$Rooms <- as.integer(housing_dataset$Rooms)
housing_dataset$Bathroom <- as.integer(housing_dataset$Bathroom)
housing_dataset$Car <- as.integer(housing_dataset$Car)
```

```{r}
str(housing_dataset$Rooms)
summary(housing_dataset$Rooms)
```

```{r}
str(housing_dataset$Bathroom)
summary(housing_dataset$Bathroom)
```

```{r}
str(housing_dataset$Car)
summary(housing_dataset$Car)
```

## YearBuilt Column

We will format “YearBuilt” column to integer. Since we don't have any month and day information in this column,
we don't need to use it as Date object.
After formatting, we will be checking the structure and summary of this column.
As we can see from summary report below, we have 19306 missing values in this column.

```{r, warning=FALSE}
housing_dataset$YearBuilt <- as.integer(housing_dataset$YearBuilt)
```
```{r}
str(housing_dataset$YearBuilt)
summary(housing_dataset$YearBuilt)
```

## Distance Column
We are formatting “Distance” column to numeric since we have digits in those columns.
Then we are checking structure and summary as always.
We have just 1 cell missing in “Distance” column.

```{r, warning=FALSE}
housing_dataset$Distance <- as.numeric(housing_dataset$Distance)
```

```{r}
str(housing_dataset$Distance)
summary(housing_dataset$Distance)
```

## Regionname Column
We will be factorizing the “Regionname” column and checking its levels after.
Then, we will look into structure and summary of it to be sure everything is correct.
According to summary, we have 3 cells missing in “Regionname” column.

```{r, warning=FALSE}
housing_dataset$Regionname <- factor(housing_dataset$Regionname)
```

```{r}
levels(factor(housing_dataset$Regionname))
str(housing_dataset$Regionname)
summary(housing_dataset$Regionname)
```

## PropertyCount Column
We will factorize the “PropertyCount” column because those values actually are not different from each other.
There are several property areas in different regions which has exactly the same amount of properties.
So, they show us “which” property area that property belongs to.
According to that, we will use that variable as factor, instead of integer. Then we are checking its structure and summary.
We have 3 missing rows in “PropertyCount” column, same as “Regionname” according to summaries we got.

```{r, warning=FALSE}
housing_dataset$Propertycount <- factor(housing_dataset$Propertycount)
```

```{r, results='hide'}
levels(factor(housing_dataset$Propertycount))
```

*Output of levels are not included since there are so many different levels. You can remove 'results' parameter in code block above to see output.*

```{r}
str(housing_dataset$Propertycount)
summary(housing_dataset$Propertycount)
```

# Manually Replacing Missing Values

First, let's have a look the data we have obtained during the pre-process.

Date Column: 0 missing rows
Type Column: 0 missing rows
Price Column: 7610 missing rows
Landsize Column: 11810 missing rows
BuildingArea Column: 21115 missing rows
Rooms Column: 0 missing rows
Bathroom Column: 8226 missing rows
Car Column: 8728 missing rows 
earBuilt Column: 19306 missing rows
Distance Column: 1 missing rows
Regionname Column: 3 missing rows
Propertycount Column: 3 missing rows

When we look at the table above, we can see that the Distance, Regionname and Propertycount columns have very little missing data.
Starting from this data will be the easiest step for us since they won't require any prediction. It would be more effective to use
prediction since there are only 3 missing rows and rest of the data could be used for training our model. However, the purpose of the
project is trying to apply different approaches and hands-on experiences as much as possible so we will stick with manual analysis.

```{r}
housing_dataset[is.na(housing_dataset$Distance), ]
housing_dataset[is.na(housing_dataset$Regionname), ]
housing_dataset[is.na(housing_dataset$Propertycount), ]
```

It seems that these 3 columns are missing 3 rows in common.
We have the “Distance” data in the 2 missing rows, and as we know, the “Distance”, “Regionname”, and “Propertycount” columns
are there to provide the same information: the area where the property is located.
With that in mind, we can use this “Distance” data to predict other columns.

First, let's take a look at the regions where the “Distance” data is 5.1.

```{r}
summary(housing_dataset$Regionname[housing_dataset$Distance == 5.1])
```

When we perform some mathematical operations on the above data, we obtain the Regionname value of the first row can be
Western Metropolitan with a probability of 69.1%, or Southern Metropolitan with a probability of 30.9%.
Accordingly, instead of deleting this data, we can add it to the analysis by restoring it. Using prediction models could be more accurate
with this type of probability since we could include more informations into calculation.

```{r}
housing_dataset[18524, "Regionname"] <- "Western Metropolitan"
```

Now let's do the same steps for second line and see if we can get any better probability.

```{r}
summary(housing_dataset$Regionname[housing_dataset$Distance == 7.7])
```

When we do the same operations on the second line, we get the Southern Metropolitan value with a probability of 91.9%
and a Western Metropolitan value with a probability of 8.1%.
Based on this information, we can assume the Regionname of second line to be Southern Metropolitan and include it in the analysis.

```{r}
housing_dataset[26889, "Regionname"] <- "Southern Metropolitan"
```

We can do a similar approach for the “Propertycount” column.
If we examine the property area where the properties are located in that region and include the house prices in this examination,
we can find where these properties belong.

```{r}
summary(housing_dataset$Propertycount[housing_dataset$Distance == 5.1 &
                                      housing_dataset$Regionname == "Western Metropolitan"])
```

When we look at the table above, we can say that this property is more likely to be located in the area with 7570 properties.
However, it is also useful to bring the "price" value into consideration.

```{r}
housing_dataset[18524, "Price"]
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 5.1 &
                              housing_dataset$Regionname == "Western Metropolitan" &
                              housing_dataset$Propertycount == 7570])
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 5.1 &
                              housing_dataset$Regionname == "Western Metropolitan" &
                              housing_dataset$Propertycount == 2417])
```

The value of this property is defined as 710.000 units and if we look at the mean and median price values in two different fields,
the arrows again shows the field with 7570 properties.
With this information, we edit the "Propertycount" of this property as "7570" and include it in our analysis.

```{r}
housing_dataset[18524, "Propertycount"] <- 7570
```

Now let's do the same approach for the second row.

```{r}
summary(housing_dataset$Propertycount[housing_dataset$Distance == 7.7 &
                                      housing_dataset$Regionname == "Southern Metropolitan"])
```

It is very difficult to make a prediction based on the above table. 
There are almost equal number of properties in 2 different regions.
Therefore, we will try to predict it based on the price values.

```{r}
housing_dataset[26889, "Price"]
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 7.7 &
                              housing_dataset$Regionname == "Southern Metropolitan" &
                              housing_dataset$Propertycount == 8920])
```

```{r}
summary(housing_dataset$Price[housing_dataset$Distance == 7.7 &
                              housing_dataset$Regionname == "Southern Metropolitan" &
                              housing_dataset$Propertycount == 8989])
```

The value of our second property is defined as 825.000 units.
When we look at the mean and median values in two different area, we can easily say that this property belongs to the second area.
Therefore, we change the “Propertycount” data of this property to "8989".

```{r}
housing_dataset[26889, "Propertycount"] <- 8989
```

We recovered two of the 3 missing rows. Let's take a look at the third one.

```{r}
housing_dataset[29484, ]
```

Unfortunately there is nothing we can do for this row.
While we only have the number of rooms and the price, there is no way we can recover such missing data.
Therefore, we will not include this row in our analysis.

```{r}
housing_dataset <- housing_dataset[-c(29484), ]
```

Before concluding this section, let's take a final look at a summary of our data.

```{r}
summary(housing_dataset)
```

# Outliers

Since the other columns has high amount of missing values, before doing any approach to them we will get rid of the outliers
which will help us to eliminate some of the missing values and give us an opportunity to predict some of those missing values.
With outliers, our predictions might get affected and give misleading results.

Values that we call “outlier”, which are very high or very low than the other values in the category it is in,
may affect the values we will obtain from that category and may cause us to make an incorrect analysis.
These data may be true or false, but we will exclude this data from our analysis as it will have a negative impact
on our analysis no matter what. We will make use of the quantile function and the filtering method when applying this process.

## Price Column Outliers

First of all, let's examine the data in 1% and 99% percentile and make sure there is no problem.

```{r}
quantile(housing_dataset$Price, 0.99, na.rm = TRUE)
quantile(housing_dataset$Price, 0.01, na.rm = TRUE)
```

For “Price” column we got 3.400.550 for upper and 310.000 for lower values.
Upper percentile value seems higher than expected so we might need to do something different for “Price” column.
To be sure, we will check the “Price” values in histogram plot.
To do that, we will use ggplot2 and scales package and create a histogram chart from the data we have.
To get a better chart, we increase the number of bins to 100 and focus on the X-axis between 0 and 5,000,000.
Through the Scales package, we ensure that the data on the X axis appear in a smoother format.

```{r}
library(ggplot2)
library(scales)
```

```{r, warning=FALSE}
ggplot(data = housing_dataset, aes(x = Price)) +
  geom_histogram(bins = 100) +
  coord_cartesian(xlim = c(0, 5000000)) +
  scale_x_continuous(labels = unit_format(unit = "M", scale = 1e-6))
```

According to plot we got, we can see there are prices goes up to 5 million in the plot,
so we can say that 3.400.550 is an acceptable value for price.
According to that information, we can exclude data below 1% and above 99% from our analysis.
Since we do not want to get rid of missing values yet, we include missing values in our filter by using OR operator.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Price <= quantile(housing_dataset$Price, 0.99, na.rm = TRUE) |
                                    is.na(housing_dataset$Price)), ]
housing_dataset <- housing_dataset[(housing_dataset$Price >= quantile(housing_dataset$Price, 0.01, na.rm = TRUE) |
                                    is.na(housing_dataset$Price)), ]
```
## Landsize Column Outliers

Let's first take a look at the quantile values, as we just did.

```{r}
quantile(housing_dataset$Landsize, 0.99, na.rm = TRUE)
quantile(housing_dataset$Landsize, 0.01, na.rm = TRUE)
```

In “Landsize” column, we got 2980.52 for upper value and 0 for lower value.
Now, we need to do something different because “Landsize” cannot be 0, that means there is something wrong in our data.
To be sure, we will check the distribution of “Landsize” column in histogram.

```{r, warning=FALSE}
ggplot(data = housing_dataset, aes(x = Landsize)) +
  geom_histogram(bins = nrow(housing_dataset)) +
  coord_cartesian(xlim = c(0, 500), ylim = c(0, 5000))
```

As we can see in the histogram, there are more than 2000 rows which has 0 value in Landsize row.
Because of that, we are getting 0 as lower percentile. To avoid that, we will remove the lines with 0 values in their Landsize column.

```{r}
housing_dataset <- housing_dataset[housing_dataset$Landsize > 0 | is.na(housing_dataset$Landsize), ]
```

```{r}
quantile(housing_dataset$Landsize, 0.99, na.rm = TRUE)
quantile(housing_dataset$Landsize, 0.01, na.rm = TRUE)
```

After that operation, we got 81 as lower percentile and 3439.6 as upper percentile which is acceptable.
Now we can apply the same method that we were using to “Landsize” column.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Landsize < quantile(housing_dataset$Landsize, 0.99, na.rm = TRUE) |
                                    is.na(housing_dataset$Landsize)), ]
housing_dataset <- housing_dataset[(housing_dataset$Landsize > quantile(housing_dataset$Landsize, 0.01, na.rm = TRUE) |
                                    is.na(housing_dataset$Landsize)), ]
```

## BuildingArea Column Outliers

As always, we will be starting with checking percentile values.

```{r}
quantile(housing_dataset$BuildingArea, 0.99, na.rm = TRUE)
quantile(housing_dataset$BuildingArea, 0.01, na.rm = TRUE)
```

We have similar problem with “BuildingArea” column.
We are getting 484.42 as upper value but 3.43 as lower value and we know that BuildingArea cannot be 3.43,
so probably there are some incorrect values which is stacked on low values.
To approve that we will check BuildingArea column in chart.

```{r}
ggplot(data = housing_dataset, aes(x = BuildingArea)) +
  geom_histogram(bins = nrow(housing_dataset)) +
  coord_cartesian(xlim = c(0, 200), ylim = c(0, 300))
```

As we can see in the chart, there are stacking near value 0. We will zoom more to see the problem more detailed.

```{r, warning=FALSE}
ggplot(data = housing_dataset, aes(x = BuildingArea)) +
  geom_histogram(bins = nrow(housing_dataset)) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 100))
```

We can see some stacking between 0 and 7, so we will cut the values which has less than 7. After that, we can continue.

```{r}
housing_dataset <- housing_dataset[housing_dataset$BuildingArea > 5 | is.na(housing_dataset$BuildingArea), ]
```

```{r}
quantile(housing_dataset$BuildingArea, 0.99, na.rm = TRUE)
quantile(housing_dataset$BuildingArea, 0.01, na.rm = TRUE)
```

Now, we got 43.14 as lower value and 488.72 as upper value which is acceptable. Now we can apply our method on BuildingArea column.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$BuildingArea < quantile(housing_dataset$BuildingArea, 0.99, na.rm = TRUE) |
                                    is.na(housing_dataset$BuildingArea)), ]
housing_dataset <- housing_dataset[(housing_dataset$BuildingArea > quantile(housing_dataset$BuildingArea, 0.01, na.rm = TRUE) |
                                    is.na(housing_dataset$BuildingArea)), ]
```

While checking our dataset, I detected some rows which has bigger building areas than landsizes which is unrealistic.
So, we will exclude those rows aswell.

```{r}
housing_dataset <- housing_dataset[housing_dataset$BuildingArea < housing_dataset$Landsize |
                                  is.na(housing_dataset$BuildingArea) |
                                  is.na(housing_dataset$Landsize), ]
```

## Rooms Column Outliers

Let's start with checking our percentile values.

```{r}
quantile(housing_dataset$Rooms, 0.99, na.rm = TRUE)
quantile(housing_dataset$Rooms, 0.01, na.rm = TRUE)
```

When we check the quantile values for Rooms column, we we will see 5 for upper and 1 for lower percentile which is acceptable
and realistic, so we will apply the same approach but we won't include is.na() filtersince Rooms column doesn't have
any missing values in it.

```{r}
housing_dataset <- housing_dataset[housing_dataset$Rooms <= quantile(housing_dataset$Rooms, 0.99, na.rm = TRUE), ]
housing_dataset <- housing_dataset[housing_dataset$Rooms >= quantile(housing_dataset$Rooms, 0.01, na.rm = TRUE), ]
```

## Bathroom Column Outliers

Once again, we look at quantile values first.

```{r}
quantile(housing_dataset$Bathroom, 0.99, na.rm = TRUE)
quantile(housing_dataset$Bathroom, 0.01, na.rm = TRUE)
```

When we get the %99 and %1 percentile values of “Bathroom” values, it returns us 4 and 1.
We will exclude properties which has less than 1 and more than 4 bathrooms in it.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Bathroom <= quantile(housing_dataset$Bathroom, 0.99, na.rm = TRUE) | is.na(housing_dataset$Bathroom)), ]
housing_dataset <- housing_dataset[(housing_dataset$Bathroom >= quantile(housing_dataset$Bathroom, 0.01, na.rm = TRUE) | is.na(housing_dataset$Bathroom)), ]
```

## Car Column Outliers

We start with quantile as usual:

```{r}
quantile(housing_dataset$Car, 0.99, na.rm = TRUE)
quantile(housing_dataset$Car, 0.01, na.rm = TRUE)
```

In “Car” column, we get the 5 for upper percentile and 0 for lower percentile.
It's acceptable because some houses may not have car parks.
According to that we will apply the same approach to “Car” column.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$Car <= quantile(housing_dataset$Car, 0.99, na.rm = TRUE) |
                                  is.na(housing_dataset$Car)), ]
housing_dataset <- housing_dataset[(housing_dataset$Car >= quantile(housing_dataset$Car, 0.01, na.rm = TRUE) |
                                  is.na(housing_dataset$Car)), ]
```

## YearBuilt Column Outliers

We look at the quantile values one last time.

```{r}
quantile(housing_dataset$YearBuilt, 0.99, na.rm = TRUE)
quantile(housing_dataset$YearBuilt, 0.01, na.rm = TRUE)
```

We will be applying same method to YearBuilt column because we get meaningful quantile results like 1880 to 2016.

```{r}
housing_dataset <- housing_dataset[(housing_dataset$YearBuilt <= quantile(housing_dataset$YearBuilt, 0.99, na.rm = TRUE) |
                                  is.na(housing_dataset$YearBuilt)), ]
housing_dataset <- housing_dataset[(housing_dataset$YearBuilt >= quantile(housing_dataset$YearBuilt, 0.01, na.rm = TRUE) |
                                  is.na(housing_dataset$YearBuilt)), ]
```

Before we end this section, let's look at the summary of our data again and make sure that everything is as we want.

```{r}
summary(housing_dataset)
```

Everything looks good. Then we can begin to dealing with our missing data.

# Replacing Missing Values Using "mice" Package

In this section, we will mainly use the “mice” package. To speak briefly, the “mice” package implements a method to deal with missing data.
The package creates multiple imputations (replacement values) for multivariate missing data.
The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model.
The "mice" algorithm can impute mixes of continuous, binary, unordered categorical and ordered categorical data.
In addition, "mice" can impute continuous two-level data, and maintain consistency between imputations by means of passive imputation.
Many diagnostic plots are implemented to inspect the quality of the imputations.


Let's activate the “mice” package and create a “mice” object with 0 iteration.
The reason we do 0 iterations is that we don't want to do any action yet, but we want to reach the drafts that will be used
in the process to be done.

```{r, echo=FALSE}
library(mice)
init <- mice(housing_dataset, maxit = 0)
```

This “mice” object, which we have created with the name “init”, contains very different values.
Since we want to create our own method, we define the method value to an object named “meth”.

```{r}
meth <- init$method
meth
```

Here, it says which method will be used for which column. These values are the models prepared automatically by the “mice” package,
and they look fine. However, the “YearBuilt” and “BuildingArea” columns in our data contain a large amount of missing data.
Therefore, we do not want to make a prediction on these. For this reason, we will leave the methods below them blank.

```{r}
meth[c("YearBuilt", "BuildingArea")] <- ""
```

Then we will use the “quickpred” function of the “mice” package to determine which columns will be used as predictors for which data.

```{r}
pred_m <- quickpred(housing_dataset, mincor = 0.2)
pred_m
```

As we just mentioned, “YearBuilt” and “BuildingArea” contain a lot of missing data, so it can negatively affect the predictions
and even cause some data to be unpredictable. That's why we take them out of this matrix too.

```{r}
pred_m[, c("Date", "YearBuilt", "BuildingArea")] <- 0
```

We are now ready to create our model. We create a “mice” object using the variables we created and our data and start predicting.
Then we complete the created model and define it to our data.

```{r}
imp <- mice(data = housing_dataset, method = meth, predictorMatrix = pred_m)
housing_dataset <- complete(imp)
```

Finally, we drop the “Yearbuilt” and “BuildingArea” columns from our table and take a look at the summary of our data.

```{r}
housing_dataset$BuildingArea <- NULL
housing_dataset$YearBuilt <- NULL
summary(housing_dataset)
```

Since we don't have any missing data, our data is ready for analysis!

# Analysis & Visualization

```{r}
summary(housing_dataset)
```

In this section, we will try to make an analysis on all the columns we have and try to use as much as
different data visualization method that we can use. First, let's take a look at the data we have.

The “Date” column tells us that this data covers sales from January 2016 to March 2018.

When we look at the “Type” column, we can say that a large number of House sales were made.
The number of Houses sold is almost 3 times the Townhouse and Unit/Duplex combined.

When we look at the mean and median values in the “Price” column, we can say that the properties sold are worth approximately 1 million units, 
but spread to 310,000 to 3,400,000 units.

The “Landsize” column says that on average 550-560 units of properties are sold, but there is still a large distribution.

The “Rooms”, “Bathroom”, and “Car” columns give us information about the size and variety of houses in general.

When we look at the “Distance” data, it shows that our data includes properties located directly in the center and
at a location far fromthe center.

The “Regionname” category indicates that while sales are mostly made in metropolitan areas,
we do have areas with few properties, such as Eastern Victoria.

Finally, when we look at the values in the “Propertycount” column, it shows that sales are made in many different property areas.
We can use those information in further analysis.

## Property Type Distribution (Pie Chart)

We can use the pie chart to show the difference in the “Type” column. First, let's find the percentage values of House,
Unit/Duplex and Townhouse one by one and define them into a variable.

Then we will create a pieLabels to use in the colors and the data to be used, and the pieChartData variable to use it as data.

Since ggplot does not support pie chart directly, we will solve this by using a different method at this point.
Of course, we can use R's own pie chart function, but since we will do other charts via ggplot, I choose to stick with this package.
For this, we will first create a ggplot object. We will use the "piechart_data" we created for the data layer.

For the Aesthetic layer, we will leave x blank and assign "pie_slices" to y. We will assign the "pie_labels" that we created to the “fill” parameter.
This creates our main ggplot object. Then we will be creating a bar object for the geometry layer.

In order to make barplot look like a pie chart, we will spread the bars we have around a point by 360 degrees
by using coord_polar. Then we will set the desired naming and colors and our pie chart will be ready!

```{r}
house_percentage <- nrow(housing_dataset[housing_dataset$Type == "House", ]) / nrow(housing_dataset)
ud_percentage <- nrow(housing_dataset[housing_dataset$Type == "Unit/Duplex", ]) / nrow(housing_dataset)
townhouse_percentage <- nrow(housing_dataset[housing_dataset$Type == "Townhouse", ]) / nrow(housing_dataset)

pie_slices <- c(house_percentage, ud_percentage, townhouse_percentage)
pie_labels <- c("House", "Unit/Duplex", "Townhouse")
piechart_data <- data.frame(pie_labels, pie_slices)

ggplot(data = piechart_data, aes(x = "", y = pie_slices, fill = pie_labels)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Property Type Distribution") +
  labs(fill = "Types")
```

## Property Distribution per Region (Bar Chart)

We do not need to do any pre-processing while creating a bar chart. So we will start directly with the ggplot object.

We will assign our data to the data layer, the “Regionname” column to the y coordinate of the aesthetic layer,
and the “Type” column to the fill parameter for coloring.

Then we will create our bar chart with the geom_bar() function and define the position parameter as “fill”.
This is because in some areas there are almost no Townhouses and Units/Duplexes, so the “fill” position will make them look more readable.
However, it uses percentage instead of count while doing this.

Finally we will set the width to 0.8 purely for display purposes. After the desired theme and naming, our bar chart will be ready.

```{r}
ggplot(data = housing_dataset, aes(y = Regionname, fill = Type)) +
  geom_bar(position = "fill", width = 0.8) +
  theme_bw() +
  labs(title = "Property Distribution per Region") +
  labs(x = "Percentage") + labs(y = "Region Name")
```

## Landsize Distribution (Histogram)

Creating a histogram is easier than others. We will create our ggplot object with our data and aesthetic values,
and then we will add the histogram geometry to this object with the geom_histogram() function.

It is important to set the number of bins correctly. Too high bin number will seem visually crowded, and
too low may create a deficiency in the information it offers.
We will set it to 25, thinking 25 will be appropriate.

All other layers and parameters other than this are used for visual purposes. In short, our histogram is ready.

```{r}
ggplot(data = housing_dataset, aes(x = Landsize)) +
  geom_histogram(color = "black", bins = 25, fill = "orange") +
  theme_classic() +
  labs(title = "Landsize Histogram") + labs(y = "Count")
```

## Price vs Distance per Type(Scatter Plot)

Instead of using the scatter plot alone, we can make a better presentation by using facets and geom_smooth().
So, first we will create our ggplot object and then we will be createting our scatter plot using geom_point().

At this point, the reason we lowered the alpha to 5% is to manage to visualize the clutter on the chart.
Because we are using a lower alpha, the places with agglomeration will appear with a brighter color,
while as the agglomeration decreases, we will see a dimmer color.

We will use the facet_grid() function to present different property types. As we've used it before,
we will be using the “scales” package to get smoother numbers, and instead of using 1e-16 to represent million,
we will make the letter “M” appear. After adding our trend line to the scatter plot using geom_smooth(),
we will make our final visual adjustments. Scatter plot ready!

```{r}
ggplot(data = housing_dataset, aes(x = Distance, y = Price)) + geom_point(aes(color = Type), size = 2.5, alpha = 0.05) +
  facet_grid(rows = housing_dataset$Type) +
  scale_y_continuous(labels = unit_format(unit = "M", scale = 1e-6)) +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs")) +
  theme(legend.position = "none") +
  labs(title = "Price Distribution in Distance to City Center")
```

## Price Analysis

### Histogram

As always when creating this histogram, after creating the ggplot object, we will be creating our histogram via geom_histogram().
Using geom_vline() and annotate(), we will place mean, median and standard deviation values in our histogram.

After the desired naming and coloring, we see a table like this:

```{r}
ggplot(data = housing_dataset, aes(x = Price)) +
  geom_histogram(color = "black", bins = 25, fill = "beige") +
  geom_vline(xintercept = round(mean(housing_dataset$Price)), color = "cyan3", size = 1) +
  geom_vline(xintercept = round(median(housing_dataset$Price)), color = "purple", size = 1) +
  geom_vline(xintercept = round(mean(housing_dataset$Price) + sd(housing_dataset$Price)), color = "darkseagreen4", size = 1) +
  geom_vline(xintercept = round(mean(housing_dataset$Price) - sd(housing_dataset$Price)), color = "darkseagreen4", size = 1) +
  annotate(geom = "label", x = round(mean(housing_dataset$Price)), y = 4800, label = paste("ST-Dev: ±", round(sd(housing_dataset$Price))), fill = "darkseagreen4", size = 3.5) +
  annotate(geom = "label", x = round(mean(housing_dataset$Price)) + 360000, y = 3800, label = paste("Mean:", round(mean(housing_dataset$Price))), fill = "cyan3", size = 3.5) +
  annotate(geom = "label", x = round(median(housing_dataset$Price)) + 370000, y = 4300, label = paste("Median:", round(median(housing_dataset$Price))), fill = "purple", size = 3.5) +
  scale_x_continuous(breaks = round(seq(0, 5000000, by = 250000), 1), labels = unit_format(unit = "", scale = 1e-6)) +
  labs(title = "Price Histogram") + labs(y = "Count") + labs(x = "Price (Million)")
```

When we look at our table, we can see why the median value is sometimes preferred more than the mean value.
In this right-skewed (positive skeweness) histogram, we can see that although the density is on the left side,
the low-frequency but high-value properties on the right side increase the mean value.

If we examine our table, we can argue that the properties in our dataset are mostly valued between 500,000 and 1,000,000 units.
When we look at the standard deviation value, choosing properties that we can evaluate between 500,000 and 1,600,000 units for
future property constructions will secure our sales in the coming years. Since the sales of the houses outside this range are very few,
the sales process will be much longer, which may cause disruption in our future planning. Of course, before making this decision,
the expense spent on these properties should also be considered, but the data we have does not cover this.
By doing a detailed research on Melbourne and Australia, we can find out how the property prices in Melbourne compare to the
rest of the country, and inferences can be made about the average income status of people in the continuation of this research.

### Grouping Properties by Price

For this grouping, instead of choosing random values, we will be using the 1st and 3rd quantile data.
We will be defining properties with a price less than 1st quantile as low-priced houses, properties with a higher than 3rd quantile
as high-priced houses, and properties between these two values as medium-priced houses.

For this, first of all, let's check the 25% and 75% values, which we call 1st and 3rd quantile, using the quantile function.

```{r}
quantile(housing_dataset$Price, 0.25)
quantile(housing_dataset$Price, 0.75)
```

Then let's use this data as a filter and define new variables.

```{r}
lowpriced_properties <- housing_dataset[housing_dataset$Price < quantile(housing_dataset$Price, 0.25), ]
midpriced_properties <- housing_dataset[housing_dataset$Price >= quantile(housing_dataset$Price, 0.25) &
                                       housing_dataset$Price <= quantile(housing_dataset$Price, 0.75), ]
highpriced_properties <- housing_dataset[housing_dataset$Price > quantile(housing_dataset$Price, 0.75), ]
```

Let us now examine the summaries of these groups separately.

```{r}
summary(lowpriced_properties)
```

When we look at low-priced properties, we see that there are mostly House and Unit/Duplex properties.
Almost all Unit/Duplex type properties are gathered in Low-priced properties. It is still early to predict the reason for this,
we need to check other data and examine other price groups as well. On average, there are 2-3 rooms, 1 bathroom and 1-2 car parks.
In terms of distance, there are low-priced properties both in the center and at the farthest point from the center.

```{r}
summary(midpriced_properties)
```

When we look at mid-priced houses, we can see that more House type properties are in this scale. Although there is
not a big difference in the number of rooms and bathrooms, we see that the mean is lower than the median in the number of parking lots.
The reason for this may be that the houses in a certain area (for example, apartments in the city center) do not have a parking lot.
Although the distance will be the same, we can say that the properties closer to the center on average are located in mid-priced properties.

```{r}
summary(highpriced_properties)
```

When we look at the high-priced properties, we can say that almost all of them are House-type houses.
When we check the distance value, it is possible to say that the closest houses to the center are high-priced properties.
This might explain why almost all of them are House type. When we look at the number of rooms, it is seen that the number of rooms
and bathrooms is higher than other groups. This, of course, is a factor that deeply affects the price.

### Type vs Price (Boxplot)

To create this plot, we first create our ggplot object as usual. Next, we create our boxplot using geom_boxplot().
I wanted to use facet_grid() for different types of properties. Different methods can also be used.
After visual adjustments, our plot is ready.

```{r}
ggplot(data = housing_dataset, aes(x = Price, y = "")) + geom_boxplot() +
  facet_grid(rows = housing_dataset$Type) +
  scale_x_continuous(labels = unit_format(unit = "M", scale = 1e-6)) +
  labs(title = "Prices for Property Types") + labs(y = "")
```

Looking at the table, it shows that House type properties are the most expensive on average, but are distributed over a wide range.
This data is not surprising, as there are many houses in every region, at every distance.
Townhouse shows a more mid-level pricing, while the distribution between prices is lower than House-type properties.
We can see that Unit/Duplex type properties are generally cheap, but can exceed 3,000,000 units.
This visualizes the review we made in the price grouping section.

### Correlated Values

By using the cor() function for numerical values, we can measure the effect of these values on the Price function.

```{r}
round(cor(housing_dataset[, c("Price", "Landsize", "Rooms", "Bathroom", "Car", "Distance")]), 2)[, 1]
```

As we can see from the table above, most of the numerical values in our table affects the Price significantly.

And with this, we've completed our analysis on Melbourne Housing dataset. There might some updates in future to improve our approaches, add
more detailed analysis and so on. This project was made with the aim of bringing together hands-on experiences.
Although less effective methods were used in order to avoid the project's purpose at some points, the project achieved its purpose.
I hope this hands-on analysis report has helped you learn something about R, too.


```{r, include=FALSE, message=FALSE}
knitr::purl(input = "melbourne-housing.Rmd", output = "melbourne-housing-no-doc.R", documentation = 0)
file.rename(from = "melbourne-housing.md", to = "README.md")
```